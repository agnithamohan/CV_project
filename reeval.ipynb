{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from data import GanDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from probabilistic_unet import ProbabilisticUnet\n",
    "from model_helpers import getModelFilePath, getLatestModelEpoch, loadModel, saveModel\n",
    "\n",
    "MODELS_DEST = '/scratch/amr1215/probunet_models/'\n",
    "GAN_DATASET = {\n",
    "    'TRAIN': {\n",
    "        'INPUT': \"/scratch/amr1215/gan_dataset/train/inputs\",\n",
    "        'GT': \"/scratch/amr1215/gan_dataset/train/gt\"\n",
    "    },\n",
    "    'EVAL': {\n",
    "        'INPUT': \"/scratch/amr1215/gan_dataset/eval/inputs\",\n",
    "        'GT': \"/scratch/amr1215/gan_dataset/eval/gt\"\n",
    "    }\n",
    "}\n",
    "LAYER=\"fpn_res5_2_sum\"\n",
    "\n",
    "eval_dataset = GanDataset(GAN_DATASET['EVAL']['INPUT'], GAN_DATASET['EVAL']['GT'], LAYER)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "net = ProbabilisticUnet(input_channels=256, num_classes=256, num_filters=[256, 512, 1024, 2048], latent_dim=10, no_convs_fcomb=8, beta=10.0).cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "currEpoch = 0\n",
    "max_epochs = 110\n",
    "\n",
    "net.eval()\n",
    "for epoch in range(currEpoch, max_epochs, 2):\n",
    "    loadModel(net, optimizer, getModelFilePath(MODELS_DEST, LAYER, epoch))\n",
    "    eval_totalLoss = 0.0\n",
    "    eval_losses = {\n",
    "        'rec': [],\n",
    "        'kl': [],\n",
    "        'l2pos': [],\n",
    "        'l2pri': [],\n",
    "        'l2fcom': [],\n",
    "        'total': []\n",
    "    }\n",
    "    eval_targetLosses = []\n",
    "    eval_count = 0\n",
    "    betterCount = 0\n",
    "    for idx, data in enumerate(eval_loader):\n",
    "    #if idx == 2:\n",
    "    #    break\n",
    "        #print(\"EVAL idx:\", idx)\n",
    "        inp = data[\"input\"][0].cuda()\n",
    "        gt = data[\"gt\"][0].cuda()\n",
    "        lossBaseline = torch.nn.L1Loss()(inp, gt).item()\n",
    "        print(\"Target Loss:\", lossBaseline)\n",
    "        if(torch.isnan(lossBaseline)):\n",
    "            continue\n",
    "        net.forward(inp, gt, training=False)\n",
    "        reconLoss, klLoss = net.elbo(gt,training=False)\n",
    "        loss = reconLoss\n",
    "        print(\"Total Loss: \", loss.item())\n",
    "        if(loss.item() < lossBaseline):\n",
    "            betterCount += 1\n",
    "        eval_losses['rec'].append(reconLoss.item())\n",
    "        eval_losses['kl'].append(0.0)\n",
    "        eval_losses['l2pos'].append(0.0)\n",
    "        eval_losses['l2pri'].append(0.0)\n",
    "        eval_losses['l2fcom'].append(0.0)\n",
    "        eval_losses['total'].append(loss.item())\n",
    "        eval_targetLosses.append(lossBaseline)\n",
    "        eval_count += 1\n",
    "        eval_totalLoss += loss.item()\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"EVAL Total Loss: \", eval_totalLoss)\n",
    "    print(\"EVAL Average Loss: \", eval_totalLoss/eval_count)\n",
    "    print(\"EVAL Better than Target Loss: \", betterCount, \"/\", eval_count)\n",
    "    with open(getModelFilePath(MODELS_DEST, LAYER, epoch).split('.')[0]+'_eval_loss', 'wb') as f:\n",
    "        pickle.dump({'losses': eval_losses, 'targetLosses': eval_targetLosses, 'count': eval_count, 'betterCount': betterCount}, f)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (instpred)",
   "language": "python",
   "name": "instpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
